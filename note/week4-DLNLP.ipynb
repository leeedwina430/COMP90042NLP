{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for NLP: Feed forward Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward Neural Networks Basics\n",
    "Deep Learning: re-branded name for neural networks, a banch of machine learning\n",
    "\n",
    "Deep: refers to many layers that are chained together in a model\n",
    "\n",
    "**Neurons**: computation units\n",
    "\n",
    "\n",
    "### Feed-forward NN\n",
    "<font color=red>AKA multilayer perceptrons (多层感知机)</font>\n",
    "\n",
    "**Arrow**: carries weights; reflecting importance\n",
    "\n",
    "Certain layers have non-linear activation functions (e.g. softmax)\n",
    "\n",
    "<center>\n",
    " <img src=\"./figures/week3l1-1.png\" width = \"250\" alt=\"图片名称\" align=center />\n",
    "</center>\n",
    "\n",
    "\n",
    "### Neuron\n",
    "each neuron is a function: $h=tanh(\\sum_jw_jx_j+b)$\n",
    "\n",
    "scales input (with $w$), and adds offset (bias $b$)\n",
    "\n",
    "**Parameters**: $w$ and $b$s.\n",
    "\n",
    "#### Matrix Vector Notation\n",
    "\n",
    "$$h_i=func(\\sum_jw_{ij}x_j+b_i)$$\n",
    "\n",
    "$$\\bar{h}=func(W\\bar{x}+\\bar{b})$$\n",
    "\n",
    "where $W$ is a matrix comprising the weight vectors, $b$ is a vector of all bias terms\n",
    "\n",
    "**None-linear function applied element-wise**\n",
    "\n",
    "\n",
    "### Outputlayer\n",
    "\n",
    "1. Binary classification: (+,-) 1 neuron with sigmoid activation function\n",
    "2. Multi-class classification: softmax ensures probabilities $> 0$ and sum to $1$ (<font color=red>AKA discrete probabilities.</font>)\n",
    "$$\\left[\\frac{\\exp(v_1)}{\\sum_i\\exp(v_i)},\\frac{\\exp(v_2)}{\\sum_i\\exp(v_i)},\\dots,\\frac{\\exp(v_m)}{\\sum_i\\exp(v_i)}\\right]$$\n",
    "\n",
    "3. Continuous Probability?: <font color=red>MDN</font>\n",
    "4. Regression problems?\n",
    "\n",
    "\n",
    "### Learning from Data (for classification)\n",
    "maximise the total probability $L$\n",
    "$$L=\\prod_{i=1}^mP(y_i|x_i)$$\n",
    "equivalent to minimise $-\\log L$ with respect to parameters\n",
    "$$\\iff -\\log(L)$$\n",
    "\n",
    "trained using Gradient descent\n",
    "\n",
    "**<font color=red>How to compute the loss?</font>**\n",
    "\n",
    "\n",
    "### Regularisation\n",
    "Many params, overfits easily; Regularisation to avoid overfitting\n",
    "\n",
    "Low bias (without any assumption), high variance (easy overfit)\n",
    "\n",
    "**Very much import in NNs**\n",
    "\n",
    "1. L1-norm: sum of absolute values of all params ($W,b$ etc.): encourage the model to split the model to all neurons.\n",
    "2. L2-norm: sum of squares\n",
    "3. Dropout: randomly zero-out some neurons of a layer (??)\n",
    "\n",
    "#### Dropout\n",
    "Set dropout rate=0.1, a random 10\\% of neurons now have 0 values\n",
    "\n",
    "mostly apply to the <font color=red>hidden layers</font>, but also any other layers\n",
    "\n",
    "e.g.\n",
    "<center>\n",
    " <img src=\"./figures/week3l1-2.png\" width = \"250\" alt=\"图片名称\" align=center />\n",
    "</center>\n",
    "\n",
    "**Works because:**\n",
    "\n",
    "It prevents the model from being over-reliant on certain neurons\n",
    "\n",
    "indirectly: It penalises large parameter weights; It introduces noise into the network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications in NLP\n",
    "\n",
    "### Topic Classification\n",
    "input: bag-of-words (document-words matrix)\n",
    "\n",
    "e.g.\n",
    "<center>\n",
    " <img src=\"./figures/week3l1-3.png\" width = \"400\" alt=\"图片名称\" align=center />\n",
    "</center>\n",
    "\n",
    "Architecture: last layer is softmax (for probability distribution)\n",
    "\n",
    "Training: Loss is cross-entropy\n",
    "\n",
    "Prediciton: Choose the argmax as current class\n",
    "\n",
    "#### Improvements\n",
    "1. bag of <font color=red>bigrams</font> as input\n",
    "2. preprocess text to lemmatise words and remove stopwords?\n",
    "3. can weight words using <font color=red>TF-IDF</font> or <font color=red>one-hot</font> vector instead of word count.\n",
    "\n",
    "\n",
    "### Language Model Revisited\n",
    "to assign a probability to a sequence of words\n",
    "\n",
    "typically, with sliding a window, to predict from finite context. e.g. $n=3$, trigram\n",
    "$$P(w_1,w_2,...,w_m)=\\prod_{i=1}^mP(w_i|w_{i-2},w_{i-1})$$\n",
    "\n",
    "Training involves collecting frequency counts (rare events → smoothing)\n",
    "\n",
    "**As Classifier**: LMs can be considered simple classifiers. e.g. trigram \n",
    "$$P(w_i|w_{i-2}=salt,w_{i-1}=and)$$\n",
    "\n",
    "\n",
    "### Feed-forward NN Language Model\n",
    "Just use NN to model the equation above.\n",
    "\n",
    "input features: the previous two words => **Embeddings** (continuous?)\n",
    "\n",
    "output class: the next word (large num of classes)\n",
    "\n",
    "\n",
    "### Word Embeddings\n",
    "Maps discrete word symbols to <font color=red>continuous vectors</font> in a __relatively low dimensional__ space\n",
    "\n",
    "help to capture similarity between words (latent semantic?)\n",
    "\n",
    "<center>\n",
    " <img src=\"./figures/week3l1-4.png\" width = \"500\" alt=\"图片名称\" align=center />\n",
    "</center>\n",
    "\n",
    "**??Question**: 是否可以不做normalisation，既然这一步骤会耗费大量的计算资源\n",
    "\n",
    "\n",
    "### Training a FFNN LM\n",
    "\n",
    "<center>\n",
    " <img src=\"./figures/week3l1-5.png\" width = \"500\" alt=\"图片名称\" align=center />\n",
    "</center>\n",
    "\n",
    "<center>\n",
    " <img src=\"./figures/week3l1-6.png\" width = \"500\" alt=\"图片名称\" align=center />\n",
    "</center>\n",
    "\n",
    "**Question**:圈加代表什么？\n",
    "\n",
    "<center>\n",
    " <img src=\"./figures/week3l1-7.png\" width = \"500\" alt=\"图片名称\" align=center />\n",
    "</center>\n",
    "</br>\n",
    "<center>\n",
    " <img src=\"./figures/week3l1-8.png\" width = \"500\" alt=\"图片名称\" align=center />\n",
    "</center>\n",
    "\n",
    "\n",
    "### Advantages of FFNN LM\n",
    "Count-based $N$-fram models:\n",
    "\n",
    "1. Cheap to train\n",
    "2. problems with sparsity & novelty (scaling to larger contexts)\n",
    "3. won't adequately capture properties of words (e.g. grammatical and semantic similarity)\n",
    "\n",
    "pros of FFNN $N$-gram models:\n",
    "\n",
    "1. automatically capture word properties -> robust estimates\n",
    "2. without any smooth! (?always give some prob to new words at random initialization)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Networks\n",
    "popular in CV; <font color=red>Identify indicative local predictors</font>; Combine them to produce a <font color=red>fixed-size</font> representation\n",
    "\n",
    "[A Beginner's Guide To Understanding Convolutional Neural Networks](https://adeshpande3.github.io/A-Beginner%27s-Guide-To-Understanding-Convolutional-Neural-Networks/)\n",
    "\n",
    "\n",
    "<center>\n",
    " <img src=\"./figures/week3l1-9.png\" width = \"500\" alt=\"图片名称\" align=center />\n",
    "</center>\n",
    "\n",
    "\n",
    "### Summary for DL in NLP (FFNL)\n",
    "Pros:\n",
    "\n",
    "1. Excellent performance\n",
    "2. less hand-engineering of features\n",
    "3. Flexible (customised architecture for different tasks)\n",
    "\n",
    "Cons:\n",
    "\n",
    "1. Much slower to train (GPU involved)\n",
    "2. Lots of params (due to vocabulary size; both input and output)\n",
    "3. Data hungry (tiny data poor performance -> pre-trained models)\n",
    "\n",
    "### Related reference\n",
    "- Feed-forward network: G15, section 4; JM Ch. 7.3-7.5 \n",
    "- Convolutional network: G15, section 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Networks\n",
    "\n",
    "### N-gram Language Models\n",
    "\n",
    "pros:\n",
    "1. Can be implemented using counts (with smoothing)\n",
    "2. Can be implemented using feed-forward neural networks (word embedding)\n",
    "3. Generates sentences like (trigram model) : `I saw a table is round and about`\n",
    "\n",
    "Prolems: **limited context**\n",
    "\n",
    "\n",
    "### Recurrent Neural Networks (RNN)\n",
    "pros:\n",
    "1. Allow representation of arbitrarily sized inputs (n-1 long)\n",
    "\n",
    "Idea: process the input sequence one at a time, by applying a recurrence formula\n",
    "\n",
    "**State Vector**: ty represent contexts that have been previously processed\n",
    "\n",
    "$$s_i=f(s_{i-1},x_i)$$\n",
    "$$s_{i+1}=f(s_{i},x_{i+1})$$\n",
    "\n",
    "$f$-recurring function (use this all the time), \n",
    "\n",
    "<center>\n",
    " <img src=\"./figures/week3l2-1.png\" width = \"500\" alt=\"图片名称\" align=center />\n",
    "</center>\n",
    "just adds some non-linear layers\n",
    "\n",
    "$$s_i=tanh(W_ss_{i-1}+W_xx_i+b)$$\n",
    "\n",
    "if $x_i$ is a one-hot vector, then $W_x$ is just the word embedding\n",
    "\n",
    "\n",
    "### Simle RNN\n",
    "\n",
    "$$s_i=tanh(W_ss_{i-1}+W_xx_i+b)$$\n",
    "$$$$\n",
    "\n",
    "<center>\n",
    " <img src=\"./figures/week3l2-1.png\" width = \"500\" alt=\"图片名称\" align=center />\n",
    "</center>\n",
    "\n",
    "$(W_s,W_x,b,W_y)$ are used across all time steps, so not many params\n",
    "\n",
    "#### Simple RNN Training\n",
    "\n",
    "\n",
    "### (Simple) RNN for Language Model\n",
    "\n",
    "#### RNN Language Model Training\n",
    "\n",
    "<center>\n",
    " <img src=\"./figures/week3l2-1.png\" width = \"500\" alt=\"图片名称\" align=center />\n",
    "</center>\n",
    "\n",
    "once finish the training sentence, we'll then sum all the loss up, and back propagation.\n",
    "\n",
    "#### RNN Language Model Generation\n",
    "\n",
    "<center>\n",
    " <img src=\"./figures/week3l2-1.png\" width = \"500\" alt=\"图片名称\" align=center />\n",
    "</center>\n",
    "\n",
    "#### RNN Generation Prolems\n",
    "1. mismatch between training and decoding (ighest propability won't always give you a good choice)\n",
    "2. error propagation in intermediate steps will never recover\n",
    "2. trends to generate \"bland\" or \"generic\" language\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long Short-term Memory Networks (LSTM)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c8f9177a0acce8018d23d7772672ff7f2c1807cf103258a4b51e26a443b2e37b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
