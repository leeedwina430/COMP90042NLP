{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fundamentals of classification\n",
    "\n",
    "<center>\n",
    "<img src=\"./figures/week2l1-4.png\" width = \"400\" alt=\"图片名称\" align=center />\n",
    "</center>\n",
    "\n",
    "input: document $d$ (a vector of features); output set of classes $C=\\{c_1,c_2,...c_k\\}$\n",
    "\n",
    "output: A predicted class $c\\in C$\n",
    "\n",
    "<font color=red>NOTE: most classification need feature enginerring</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text classification tasks\n",
    "\n",
    "e.g.: Topic classification, sentiment analysis, Native-language identification, natural language inference, automatic fact-checking (?)\n",
    "\n",
    "input may not be a long document (e.g. sentence, tweet-level)\n",
    "\n",
    "\n",
    "### Topic Classification\n",
    "Motivation: library science (categorize them), information retrival\n",
    "\n",
    "Classes: Topic categories (e.g. \"jobs\",\"international news\")\n",
    "\n",
    "Features: \n",
    "1. Unigram (bag of words, BOW) (<font color=red>with stop-words removed</font>)\n",
    "2. N-grams (for phrases)\n",
    "\n",
    "Examples of corpora: Reurers news corpus (RCV1??,NLTK), Pubmed abstracts (出版摘要), Tweets with hashtags\n",
    "\n",
    "\n",
    "### Sentiment Analysis\n",
    "Motivation: opinion mining, business analytics\n",
    "\n",
    "Classes: Positive / Negative / Neutral\n",
    "\n",
    "Features: N-grams (<font color=red>don't remove stop words...or BOW?</font>), Polarity lexicons (积极/消极词汇的集合-手工涉定)\n",
    "\n",
    "Examples of corpora: Movie review dataset, SEMEVAL (??) twitter polarity datasets\n",
    "\n",
    "\n",
    "### Native-Language Identification\n",
    "Motivation: forensic linguistics, educational applications\n",
    "\n",
    "Classes: first language of author (e.g. Indonesian)\n",
    "\n",
    "Features: Word (?) N-gram, Syntactic patterns (POS, parse trees), Phonological features (语音特征)\n",
    "\n",
    "Examples of corpora: TOEFL/IELTS essay corpora\n",
    "\n",
    "\n",
    "### Natural Language Inference (textual entailment 蕴含)\n",
    "Motivation: language understanding (the relationship between sentences)\n",
    "\n",
    "Classes: entailment, contradiction, neutral\n",
    "\n",
    "Features: Word overlap, Length difference between the sentences (?), N-grams (<font color=red>not remove stop word</font>)\n",
    "\n",
    "Examples of corpora: SNLI, MNLI\n",
    "\n",
    "\n",
    "### Building a Text Classifier\n",
    "1. Identify a task of interest\n",
    "2. Collect an appropriate corpus (or you can build it)\n",
    "3. Carry out annotation\n",
    "4. Select features\n",
    "5. Choose a machine learning algorithm (SVM?)\n",
    "6. Train model and tune hyper-parameters using held-out development data\n",
    "7. Repeat earlier steps as needed (noisy / small dataset)\n",
    "8. Train final model (never seen examples)\n",
    "9. Evaluate model on held-out test data (avoid over-fitting)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithms for classification\n",
    "\n",
    "### Choosing a Classification Algorithm\n",
    "\n",
    "Bias: assumptions we made in our model (more assump, more bias)\n",
    "\n",
    "Variance: sensitivity to training set (high-over fit)\n",
    "\n",
    "e.g. underlying assumptions (e.g. independence in Naive Bayes)\n",
    "\n",
    "Trade-off between complexity and speed.\n",
    "\n",
    "### Naive Bayes\n",
    "highest likelihood under Bayes law\n",
    "$$P(C|F)\\in P(F|C)P(C)$$\n",
    "\n",
    "$P(C)$-class prob\n",
    "\n",
    "$P(F|C)$-prob of eatures given the class\n",
    "\n",
    "Assumption: independence (can't be satisfied in most cases)\n",
    "$$P(c_n|f_1,...f_m)=\\prod_{i=1}^mP(f_i|c_n)p(c_n)$$\n",
    "\n",
    "Pros:\n",
    "1. Fast to train and classify\n",
    "2. robust, low-variance -> <font color=red>good for low data situations</font>\n",
    "3. optimal classifier if independence assumption is correct\n",
    "4. extremely simple to implement\n",
    "\n",
    "Cons:\n",
    "1. Independence assumption rarely holds (High bias)\n",
    "2. low accuracy compared to similar methods in most situations\n",
    "3. smoothing required for unseen class/feature combinations (frequency of the class; typical smoothing techniques)\n",
    "\n",
    "\n",
    "### Logistic Regression\n",
    "Linear model, uses $softmax$ \"squashing\" to get valid prob\n",
    "\n",
    "$$p(c_n|f_1,...f_m)=\\frac{1}{Z}\\exp(\\sum_{i=1}^mw_if_i)$$\n",
    "\n",
    "$Z$-normalisation factor\n",
    "\n",
    "Training maximizes probability of training data subject to <font color=red>regularization</font> which encourages low or sparse weights (easilty over-fit)\n",
    "\n",
    "\n",
    "Pros:\n",
    "1. not confounded by diverse, correlated features (better performance)\n",
    "2. can look at weights for interpretability\n",
    "\n",
    "Cons:\n",
    "1. Slow to train \n",
    "2. feature scaling needed (to similar matitude????) should learn a low weight????\n",
    "3. Requires a lot of data to work well in pracice\n",
    "4. Choosing regularisation strategy is important since overfitting easily\n",
    "\n",
    "\n",
    "### Support Vector Machines\n",
    "Finds hyperplane which separates the training data with maximum margin\n",
    "\n",
    "Pros:\n",
    "1. Fast and accurate linear classifier (for NLP tasks)\n",
    "2. Can do non-linearity with kernel trick\n",
    "3. Workd well with huge feature sets\n",
    "\n",
    "Cons:\n",
    "1. Multiclass classification awkward (but you can still work with it ???)\n",
    "2. Feature scaling needed\n",
    "3. Deals poorly with class imbalances\n",
    "4. Interpretability\n",
    "\n",
    "NLP problems often involve <font color=red>large feature sets</font>, Prior to deep learning, SVM is very \n",
    "popular for NLP\n",
    "\n",
    "### K-Nearest Neighbour\n",
    "Classify based on majority class of k-nearest training examples in feature space\n",
    "\n",
    "Definition of nearest: Euclidean distance, Cosine distance\n",
    "\n",
    "Pros:\n",
    "1. Simple but surprisingly effective\n",
    "2. No training required\n",
    "3. Inherently multiclass\n",
    "4. Optimal classifier with infinite data\n",
    "\n",
    "Cons:\n",
    "1. Have to select k (tricky)\n",
    "2. Issues with imbalanced classes\n",
    "3. Often slow (for finding the neighbours; large feature set, then slow -> require feature engineering)\n",
    "4. Features must be selected carefully\n",
    "\n",
    "\n",
    "### Decision tree\n",
    "Construct a tree where nodes correspond to tests on individual features\n",
    "\n",
    "leaves are final class decisions\n",
    "\n",
    "based on greedy maximization of mutual information\n",
    "\n",
    "Pros:\n",
    "1. Fast to build and test\n",
    "2. Feature scaling irrelevant\n",
    "3. Good for small feature sets\n",
    "4. Handles non-linearly-separable problems\n",
    "\n",
    "Cons:\n",
    "1. not that interpretable\n",
    "2. Highly redundant sub-trees\n",
    "3. Not competitive for large feature sets (not good for nlp)\n",
    "\n",
    "\n",
    "### Ransom Forests\n",
    "ensemble classifier\n",
    "\n",
    "consists of decision trees trained on different subsets\n",
    "\n",
    "Final class decision is majority vote of sub-classifiers\n",
    "\n",
    "Pros:\n",
    "1. Usually more accurate and more robust than decision trees\n",
    "2. Great classifier for medium feature sets\n",
    "3. Training easily parallelised\n",
    "\n",
    "Cons:\n",
    "1. Interpretability\n",
    "2. Slow with large feature sets\n",
    "\n",
    "\n",
    "### Neural Networks\n",
    "An interconnected set of nodes typically arranged in layers\n",
    "\n",
    "input layer (features), output layer (class proba), and one or more hidden layers\n",
    "\n",
    "Each node performs a linear weighting of its inputs from previous layer, passes result through activation function to nodes in next layer\n",
    "\n",
    "Pros:\n",
    "1. Extremely powerful, custmize your architecture\n",
    "2. little feature engineering\n",
    "\n",
    "Cons:\n",
    "1. Not an off-the-shelf classifier\n",
    "2. Many hyper-parameters, difficult to optimise\n",
    "3. Slow to train (GPU)\n",
    "4. Prone to overfitting (engineering tricks??)\n",
    "\n",
    "\n",
    "### Hyper-parameter Tuning\n",
    "Development set; k-fold cross-validation (small dataset case)\n",
    "\n",
    "Specific hyper-parameters are classifier specific\n",
    "\n",
    "But many hyper-parameters relate to regularisation\n",
    "\n",
    "For multiple hyper-parameters, use <font color=red>grid search</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "### Accuracy\n",
    "\n",
    "<img src='example'>\n",
    "\n",
    "### Precision & Recall\n",
    "\n",
    "### F1-score\n",
    "\n",
    "### \n",
    "\n",
    "补一下"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c8f9177a0acce8018d23d7772672ff7f2c1807cf103258a4b51e26a443b2e37b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
