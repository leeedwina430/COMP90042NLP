{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexical Semantics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n",
    "want to compare word meaning\n",
    "\n",
    "solution:\n",
    "1. Lexical Semantics: add this information explicitly through a lexical database\n",
    "2. Distributional semantics: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexical Database\n",
    "What is meaning\n",
    "1. dictionary definition: circular problem\n",
    "2. relationships wigh other words: also circular, but better for text analysis\n",
    "\n",
    "## Definitions\n",
    "**Word sense**: one aspect of the meaning of a word. (e.g. mouse: computer, animal)\n",
    "\n",
    "**Polysemous**: word with more than one word sense\n",
    "\n",
    "Meaning Through Dictionary:\n",
    "\n",
    "**Gloss**: textual definition of a sense\n",
    "\n",
    "Meaning Through Relations:\n",
    "\n",
    "**Synonymy**: near identical meaning\n",
    "\n",
    "**Antonymy**: opposite meaning\n",
    "\n",
    "**Hypernymy**: is-a relation (cat->animal; mango->fruit)\n",
    "\n",
    "**Meronymy**: part-whole relation (leg->chair)\n",
    "\n",
    "<center>\n",
    " <img src=\"./figures/week3l1-1.png\" width = \"250\" alt=\"图片名称\" align=center />\n",
    "</center>\n",
    "\n",
    "\n",
    "### WordNet\n",
    "A database of lexical relations; available via NLTK\n",
    "\n",
    "Nodes are sense with one perticular **Gloss**, with several different lemmas: e.g. {bass6, bass voice1, basso2}, {bass1, deep6}\n",
    "\n",
    "\n",
    "**Synsets**: a cluster/set of synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['depository_financial_institution',\n",
       " 'bank',\n",
       " 'banking_concern',\n",
       " 'banking_company']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.corpus.wordnet.synsets('bank')\n",
    "nltk.corpus.wordnet.synsets('bank')[0].definition()\n",
    "nltk.corpus.wordnet.synsets('bank')[1].lemma_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypernymy Chain\n",
    "\n",
    "<center>\n",
    " <img src=\"./figures/week4l1-2.png\" width = \"250\" alt=\"图片名称\" align=center />\n",
    "</center>\n",
    "\n",
    "high enough then to share a common parent sense.\n",
    "\n",
    "That's why the senses are so fine graded (??)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Similarity\n",
    "synonymy is a binary relation, word similarity is a spectrum. \n",
    "\n",
    "can use lexical database or thesaurus to estimate word similarity.\n",
    "\n",
    "### Word Similarity with Paths\n",
    "Given WordNet, find similarity based on path length.\n",
    "\n",
    "$$pathlen(c_1,c_2)=1+edgelen$$\n",
    "$$simpath(c_1,c_2)=\\frac{1}{pathlen(c_1,c_2)}$$\n",
    "$$wordsim(w_1,w_2)=\\max_{c_1\\in sense(w_1), x_2\\in sense(w_2)}simpath(c_1,c_2)$$\n",
    "\n",
    "if look for similarity between two words: looking at the highest similarity in practice.\n",
    "\n",
    "<center>\n",
    " <img src=\"./figures/week4l1-2.png\" width = \"250\" alt=\"图片名称\" align=center />\n",
    "</center>\n",
    "\n",
    "Problems: edges vary widely in actual __semantic distance__ (much bigger near top of hierarchy)\n",
    "\n",
    "Solution1: include depth information (Wu & Palmer)\n",
    "1. use path to find lowest common subsumer (LCS)\n",
    "2. Compare using depths\n",
    "$$$$\n",
    "\n",
    "<center>\n",
    " <img src=\"./figures/week4l1-2.png\" width = \"250\" alt=\"图片名称\" align=center />\n",
    "</center>\n",
    "\n",
    "\n",
    "### Concept Probability of a node\n",
    "\n",
    "sum up uni-gram prob\n",
    "\n",
    "find more concept prob for a geological formation\n",
    "\n",
    "Abstract nodes higher in the hierarchy has a higher P(c); and diminishes really quickly\n",
    "\n",
    "### Similarity with Information Content\n",
    "\n",
    "**Information Contect**\n",
    "$$IC=-\\log P(c)$$\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Sense Disambiguation\n",
    "Task: select the correct sense for words in a sentence\n",
    "\n",
    "Baseline: assume the most popular sense\n",
    "\n",
    "### Supervised WSD\n",
    "apply standard machine classifiers\n",
    "\n",
    "input: feature vecotrs typically words and syntax around target\n",
    "\n",
    "(context may be ambiguous; decide the window size(in practice small))\n",
    "\n",
    "Problem: require sense-tagged corpora (nltk; time consuming to create)\n",
    "\n",
    "### Unsupervised: Lesk\n",
    "Lesk: choose sense whose wordNet gloss overlaps most with the context.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributional Semantics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexical Database Problems\n",
    "1. Manually constructed (expensive, biased & noisy)\n",
    "2. Language is dynamic (new words, senses)\n",
    "\n",
    "-> use massive text\n",
    "\n",
    "## Distributional Hypothesis\n",
    "idea: You shall know a word by the company it keeps\n",
    "1. Document co-occurrence indicative of **topic** (document context; e.g. voting and politics)\n",
    "2. local context reflects its meaning (window context; e.g. learn smaller...)\n",
    "\n",
    "## Guessing Meaning from Context\n",
    "1. learn unkown word from its usage\n",
    "2. look at words that share similar contexts\n",
    "\n",
    "\n",
    "## Word Vectors\n",
    "captures distributional properties of a word; Semantic relationships (synonymy, analogy..)\n",
    "\n",
    "e.g. Word Embeddings ???(by-product); Count-Based methos; Neural methods (Word2Vec) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Couunt-based methods\n",
    "document context v.s. neighbouring words context\n",
    "\n",
    "### Vector Space Model (documet context)\n",
    "Idea: represent word meaning as a vector\n",
    "\n",
    "Matrix: two viewpoints......\n",
    "\n",
    "....\n",
    "\n",
    "1. weighting the values\n",
    "2. Creating low-dimensional dense vectors\n",
    "\n",
    "### Document Context\n",
    "\n",
    "#### Tf-idf\n",
    "idea: downweight the frequencies of frequent words / discounts common words!\n",
    "$$idf_{w}=\\log\\frac{|D|}{df_{w}}$$\n",
    "\n",
    "where $|D|$ means total #docs, $df_{w}$ represents #docs that has $w$\n",
    "\n",
    "**Dimenstionality Reduction**\n",
    "Problem:  Term-document matrices are very **sparse**\n",
    "\n",
    "pros: practical; remove noise; Dimensionality reduction\n",
    "\n",
    "**Singular Value Decomposition**\n",
    "$$A=U\\Sigma V^T$$\n",
    "\n",
    "$A\\in R^{|D|\\times |V|}$-tf-idf matrix, $|D|$-#words in doc, \n",
    "\n",
    "**Truncating: Latent Semantic Analysis**\n",
    "information is ranked by importance in the term matrix.\n",
    "\n",
    "A kind of way to learn word vectors.\n",
    "\n",
    "$k\\approx100$\n",
    "\n",
    "\n",
    "### Words as Context\n",
    "a window of $N$ words; lists how often words appear with other words\n",
    "\n",
    "Word-word matrix.\n",
    "\n",
    "Problem: dominated by common words.\n",
    "\n",
    "#### Pointwise Mutual Information\n",
    "PMI computes the discrepancy between $x$ and $y$.\n",
    "\n",
    "Assume independence $P(x,y)=P(x)P(y)$\n",
    "\n",
    "$$PMI(x,y)=\\log_2\\frac{P(x,y)}{P(x)P(y)}$$\n",
    "\n",
    "<font color=red>注意这里是以2为底的log！！</font>\n",
    "\n",
    "....\n",
    "\n",
    "Higher $PMI$ represents higher co-meaning?\n",
    "\n",
    "1. does a better job of capturing semantics\n",
    "2. very biased toward rare word pair\n",
    "3. and doesn't handle zeros well (-inf)\n",
    "\n",
    "some tricks\n",
    "1. Positive PMI: zero all negative values \n",
    "2. Normalised PMI: counter bias towards rare events\n",
    "\n",
    "...\n",
    "\n",
    "**Singular Value Decomposition** is still applicable. (for all matrix representations)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "问一下Tf-idf现在还会使用吗？\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural methods\n",
    "\n",
    "### Word Embeddings\n",
    "word vectors are just by-products.\n",
    "\n",
    "### Word2Vec \n",
    "An unsupervised and efficient neural network designed to get word vectors; really popular in preprocessing.\n",
    "\n",
    "Framed as learning a classifier.\n",
    "\n",
    "**Skip-gram**:  predict surrounding words of target word\n",
    "\n",
    "**CBOW**: predict target word using surrounding words\n",
    "\n",
    "...\n",
    "\n",
    "\n",
    "#### Skip-gram\n",
    "\n",
    "$$L(\\theta)=\\sum_{(t,c)\\in +}\\log P(+|t,c)+\\sum_{(t,c)\\in -}\\log P(-|t,c)$$\n",
    "\n",
    "#### Negative Sampling\n",
    "\n",
    "$$L(\\theta)=\\log P(+|t,c)+\\sum_{i=1}^k\\log P(-|t,n_i)$$\n",
    "\n",
    "just to avoid softmax over full vocabulary\n",
    "\n",
    "pros:\n",
    "1. unsupervised\n",
    "2. efficient (with negative sampling)\n",
    "3. scale to very very large corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "### Word Similarity\n",
    "measure similarity (like cosine...)\n",
    "\n",
    "compare predicted similarity with human intuition (xxx; datasets: ...)\n",
    "\n",
    "\n",
    "### Word Analogy\n",
    "$$v(Man) - v(King) = v(Woman) - v(???)$$\n",
    "\n",
    "Embedding Space shows interesting geometry\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c8f9177a0acce8018d23d7772672ff7f2c1807cf103258a4b51e26a443b2e37b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
