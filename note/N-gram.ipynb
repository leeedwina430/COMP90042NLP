{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-gram Language Models\n",
    "\n",
    "## Language Models\n",
    "\n",
    "used in modeling fluency (speech recognition); measure goodness using probabilities; used for generation (ChatGPT); query completion, optical character recognition\n",
    "\n",
    "1. machine translation\n",
    "2. summarisation\n",
    "3. dialogue systems\n",
    "\n",
    "pretrained language models are the backbone of modern NLP systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "w## Deriving n-gram language models\n",
    "\n",
    "### Probabilities: Joint to Conditional\n",
    "\n",
    "First step: apply the chain rule\n",
    "\n",
    "$$P(w_1,w_2,...,w_m)=P(w_1)P(W_2|W_1)P(w_3|w_1,w_2)...P(w_m|w_1,...,w_{m-1})$$\n",
    "\n",
    "### The Markov Assumption\n",
    "\n",
    "$$P(w_i|w_1,...,w_{i-1})\\approx P(w_i|w_{i-n+1},...,w_{i-1})$$\n",
    "\n",
    "when $n=1$, unigram: $P(w_1,w_2,...,w_m)=\\prod_{i=1}^mp(w_i)$\n",
    "\n",
    "when $n=2$, bigram: $P(w_1,w_2,...,w_m)=\\prod_{i=2}^mp(w_i|w_{i-1})$\n",
    "\n",
    "when $n=3$, unigram: $P(w_1,w_2,...,w_m)=\\prod_{i=3}^mp(w_i|w_{i-1},w_{i-2})$\n",
    "\n",
    "### Maximum Likelihood Estimation\n",
    "\n",
    "unigram: $P(w_i)=\\frac{C(w_i)}{M}$, $M$ is the total number of the word tokens in corpus.\n",
    "\n",
    "bigram: $P(w_i|w_{i-1})=\\frac{C(w_{i-1},w_i)}{C(w_{i-1})}$\n",
    "\n",
    "n-gram: $P(w_i|w_{i-n+1},...,w_{i-1})=\\frac{C(w_{i-2},w_{i-1},w_i)}{C(w_{i-2},w_{i-1})}$\n",
    "\n",
    "### Book-ending Sequence\n",
    "\n",
    "denote start and end of sequence\n",
    "\n",
    "`<s>` = sentence start\n",
    "\n",
    "`</s>` = sentence end\n",
    "\n",
    "### Trigram example\n",
    "e.g.\n",
    "$$P(yes,no,no,yes)=P(yes|<s><s>)\\times\\\\ P(no|<s>yes)\\times P(no|yes,no)\\times P(yes|no,no)\\times P(</s>|no,yes)$$\n",
    "\n",
    "<font color=red>Note: need to predict `</s>` cause it's the end of the sentence</font>\n",
    "\n",
    "$$P(w_i|w_{i-2},w_{i-1})=\\frac{C(w_{i-2},w_{i-1},w_i)}{C(w_{i-2},w_{i-1})}$$\n",
    "\n",
    "### Several Problems\n",
    "\n",
    "1. language has long distance effects => large n\n",
    "2. result probabilities are often very small (use log prob)\n",
    "3. unseen n-gram (smoothing)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smoothing to deal with sparsity\n",
    "\n",
    "### Smoothing\n",
    "\n",
    "give events you've never seen before some prob\n",
    "\n",
    "1. constricted to satisfy $P(\\{everything\\})=1$\n",
    "2. many kinds of: laplacian(add-one), add-k, absolute discounting, Kneser-Ney, Interpolation...\n",
    "\n",
    "\n",
    "### Laplacian (Add-one) Smoothing\n",
    "\n",
    "pretend we've seen each n-gram once more than we did\n",
    "\n",
    "unigram ($V$=the vocabulary, $M$ is the total number of the word tokens in corpus):\n",
    "$$P_{dll1}(w_i)=\\frac{C(w_i)+1}{M+|V|}$$\n",
    "\n",
    "bigram:\n",
    "$$P_{dll1}(w_i|w_{i-1})=\\frac{C(w_{i-1},w_i )+1}{C(w_{i-1})+|V|}$$\n",
    "\n",
    "<center>\n",
    "<img src=\"./figures/week2l1-1.png\" width = \"400\" alt=\"图片名称\" align=center />\n",
    "</center>\n",
    "\n",
    "<font color=red> NOTE: `<s>` is not part of vocabulary: cause we never need to infer its conditional prob (e.g. P(`<s>`|...)); But `</s>` is included. </font>\n",
    "\n",
    "just give too much prob\n",
    "\n",
    "\n",
    "### Add-k($\\alpha$) smoothing (Lidstone Smoothing)\n",
    "\n",
    "add just one is often too much; instead, add a fraction k: take weights from seen bigrams to unseen bigrams.\n",
    "\n",
    "$$P_{addk}(w_i|w_{i-1},w_{i-2})=\\frac{C(w_{i-2},w_{i-1},w_i)+k}{C(w_{i-2},w_{i-1})+k|V|}$$\n",
    "\n",
    "Have to choose k (tuning it); smaller $\\alpha$ means less weight to unseen.\n",
    "\n",
    "**efftective counts**: the actual (equivalent) counts that were put into this word.\n",
    "\n",
    "$$effective\\ counts=smoothed\\ prob\\times |V|$$\n",
    "\n",
    "<center>\n",
    "<img src=\"./figures/week2l1-2.png\" width = \"400\" alt=\"图片名称\" align=center />\n",
    "</center>\n",
    "\n",
    "different n-grams will give out different weights.\n",
    "\n",
    "\n",
    "### Absolute Discounting\n",
    "\n",
    "Borrow a fixed prob to unseen words, to redistribute equally. Actually calculate effective counts first, then calculates smoothed prob\n",
    "\n",
    "tune on discount $d$.\n",
    "\n",
    "<center>\n",
    "<img src=\"./figures/week2l1-3.png\" width = \"400\" alt=\"图片名称\" align=center />\n",
    "</center>\n",
    "\n",
    "\n",
    "### Katz Backoff\n",
    "Absolute discounting redistributes the probability mass equally for all unseen n-grams: not always the case; \n",
    "\n",
    "redistributes the mass based on <font color=red>a</font> **lower order** model (e.g. unigram), but just <font color=red>one order lower</font>.\n",
    "\n",
    "here, just redistribute the prob based on each word's unigram prob that occurred in unseen phrase.\n",
    "\n",
    "$$P_{katz}(w_i|w_{i-1})=\\begin{cases}\n",
    "\\frac{C(w_{i-1}, w_i)-D}{C(w_{i-1})}, & C(w_{i-1}, w_i) > 0\\\\\n",
    "\\alpha(w_{i-1})\\times\\frac{P(w_i)}{\\sum_{w_j:C(w_{i-1},w_j)=0}P(w_j)}, & otherwise\n",
    "\\end{cases}$$\n",
    "\n",
    "$\\alpha(w_{i-1})$: the amount of prob mass that has been discounted for context $w_{i-1}$ ($0.1\\times5/20$ in last figure)\n",
    "\n",
    "$P(w_i)$: unigram prob for $w_i$ (e.g. $P(infirmity)$)\n",
    "\n",
    "$\\sum_{w_j:C(w_{i-1},w_j)=0}P(w_j)$: sum unigram prob for all words that do not co-occur with context $w_{i-1}$ (e.g. $P(infirmity) + P(alleged)$); 也就是对所有没有和$w_{i-1}$一起出现过的词的概率进行求和；然后分子是其中那个我们要算的词\n",
    "\n",
    "\n",
    "如果unigram也没出现过？\n",
    "会得到0；但是如果从没出现过，也不会出现在bigram里？？？？？？？？\n",
    "\n",
    "a questional example:\n",
    "\n",
    "<center>\n",
    "<img src=\"./figures/week2l1-4.png\" width = \"400\" alt=\"图片名称\" align=center />\n",
    "</center>\n",
    "\n",
    "\n",
    "### Kneser-Ney Smoothing (Continuation prob)\n",
    "\n",
    "high versatility: co-occurs with a lot of unique words\n",
    "\n",
    "e.g. glasses: men's glasses, black glasses, buy glasses, etc\n",
    "\n",
    "e.g. francisco: san francisco\n",
    "\n",
    "$$P_{KN}(w_i|w_{i-1})=\\begin{cases}\n",
    "\\frac{C(w_{i-1}, w_i)-D}{C(w_{i-1})}, & C(w_{i-1}, w_i) > 0\\\\\n",
    "\\beta(w_{i-1})\\times P_{cont}(w_i), & otherwise\n",
    "\\end{cases}$$\n",
    "\n",
    "$$P_{cont}(w_i)=\\frac{|\\{w_{i-1}:C(w_{i-1},w_i)>0\\}|}{\\sum_{w_j:C(w_{i-1},w_j)=0}|\\{w-{j-1}:C(w_{j-1},w_j)>0\\}|}$$\n",
    "\n",
    "$\\beta(w_{i-1})$: the amount of probability mass taht has been discounted for context $w_{i-1}$ (same as $\\alpha$ in backoff)\n",
    "\n",
    "$P_{cont}(w_i)$: numerator- #unique $w_{i-1}$ that co-occurs with $w_i$ (即后一个词和前一个词一起出现的词书计数); denominator-sums all $w_j$ that do not co-occur with $w_{i-1}$ （即当前所有和前一个词没有一起出现过的备选词的数量记录）\n",
    "\n",
    "<img src=\"formula.png\", with=300>\n",
    "\n",
    "### Interpolation\n",
    "\n",
    "a better way to combine different orders of n-gram models\n",
    "\n",
    "$$P_{IN}(w_i|w_{i-1},w_{i-2})=\\lambda_3...\\lambda_2...\\lambda_1...$$\n",
    "$\\lambda_1+\\lambda_2+\\lambda_3=1$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c8f9177a0acce8018d23d7772672ff7f2c1807cf103258a4b51e26a443b2e37b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
