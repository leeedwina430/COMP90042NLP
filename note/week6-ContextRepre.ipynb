{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contextual Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WordEmbeddings don't capture multiple senses of words, regardless of the context of the word.\n",
    "\n",
    "Pretrained Contextual representations\n",
    "\n",
    "**Bidirectional RNN**: concatenate left and right representations\n",
    "\n",
    "\n",
    "<center>\n",
    "<img src=\"./figures/week2l1-4.png\" width = \"400\" alt=\"图片名称\" align=center />\n",
    "</center>\n",
    "\n",
    "## ELMo\n",
    "(Embeddings from Language Models)\n",
    "\n",
    "Bidirectional + multi-layer LSTM; combine hidden states from multiple layers rather than just top layer information.\n",
    "\n",
    "0. bidirection\n",
    "1. 2 layers of LSTM\n",
    "2. 4096 hidden dimension\n",
    "3. Character convolutional networds: to avoid unknown words\n",
    "\n",
    "low layer...\n",
    "\n",
    "higher\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT\n",
    "(Bidirectional Encoder Repressentations from Transformers)\n",
    "\n",
    "### Disadvantages of RNNs\n",
    "1. slow: sequential processing, difficult to scale to large corpus (one word a time)\n",
    "2. one-directional model: one-side; surface bidirectional representations due to simple cancatenation, & train separately\n",
    "\n",
    "\n",
    "### BERT\n",
    "1. self-attention networks (Transformers)\n",
    "2. masked language model (...)\n",
    "3. lose the abiility to generate language\n",
    "\n",
    "#### Objective 1: masked language model\n",
    "mask out k% of tokens at random; then to predict\n",
    "\n",
    "#### Objective 2: next sentence prediction\n",
    "to learn relationships between sentences; \n",
    "\n",
    "#### Training Details\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discourse\n",
    "\n",
    "understanding how sentences relate to each other in a document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discourse Segmentation\n",
    "document <- (a sequence of) <-segment (a span of cohesive text)\n",
    "\n",
    "Cohesion: organised around a topic / function\n",
    "\n",
    "\n",
    "### Unsupervised approaches\n",
    "**TextTiling algorithm**:  looking for points of low lexical cohesion between sentences\n",
    "\n",
    "for each sentence gap: \n",
    "1. 2 BOW vectors for k sentences in advance and aterward.\n",
    "2. cosine similarity score\n",
    "3. depth score i, if larger than threshold, insert boundaries\n",
    "...\n",
    "\n",
    "### Supervised Approaches\n",
    "get labelled data from easy sources (Scientific publication); looking for binary decision for each paragraph.\n",
    "\n",
    "1. binary \n",
    "2. use sequential classifiers can work better\n",
    "3. include classification of section types\n",
    "4. features include: distributional semantics, discourse markers...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discourse Parsing\n",
    "identify discourse units and their relations\n",
    "\n",
    "**Rhetorical Structure Theory (RST)**\n",
    "Hierarchical analysis of discourse structure in documents\n",
    "\n",
    "### Discourse Units\n",
    "typically clauses of a sentencee; do not cross sentence boundary (??)\n",
    "\n",
    "**Composite DU**: 2 merged DUs = another composite DU\n",
    "\n",
    "### Discourse Relations\n",
    "conjuction, justigy, concession, elaboration....\n",
    "\n",
    "### Nucleus vs. Satellite\n",
    "**Nucleus**: primary argument\n",
    "\n",
    "**Satellite**: supporting argument\n",
    "\n",
    "### RST Tree\n",
    "combines two or more DUs into composite DUs\n",
    "\n",
    "repeated process -> tree\n",
    "\n",
    "### RST Parsing\n",
    "Task: given a document, recover the RST tree\n",
    "\n",
    "e.g.: rule-based parsing, bottom-up approach, top-down approach\n",
    "\n",
    "### Discourse Markers\n",
    "some markers explicitly indicate relations. (e.g. although, but, so...)\n",
    "\n",
    "problems:\n",
    "1. many relations are not marked by markers\n",
    "2. many discourse markers ambiguous (e.g. and)\n",
    "\n",
    "\n",
    "### Machine Learning ways\n",
    "Treebank\n",
    "\n",
    "idea:\n",
    "1. segment document into DUs (supervised, not hard)\n",
    "2. combine adjacent DUs into composite DUs iteratively (bottom-up parsing)\n",
    "\n",
    "### Bottom-Up Parsing\n",
    "Transition-based parsing: greedy, uses shift-reduce algorithm\n",
    "\n",
    "CYK/chart parsing algorithm: some constraints prevent it from finding globally optimal tree...\n",
    "\n",
    "### Top-Down Parsing\n",
    "1. segment document into DUs\n",
    "2. Decide a boundary to split into 2 segments\n",
    "3. for each segment repeat ...\n",
    "\n",
    "### Features\n",
    "BOW, markers, starting/ending n-grams, location in the text, syntax feautres?, word embedding\n",
    "\n",
    "### Applications\n",
    "1. Summarisation: \n",
    "2. Sentiment Analysis: main sentence to decide\n",
    "3. Argumentation: support and arguments\n",
    "4. Authorship Attribution: perform some structure ...\n",
    "5. Essay scoring: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anaphora Resolution\n",
    "（找回指代？）\n",
    "\n",
    "### Anaphors\n",
    "linguistic expressions that refer back to earlier elements in the text\n",
    "\n",
    "**antecedent**: anaphors habe this in the discourse, often but not always a noun phrase\n",
    "\n",
    "most common: pronouns, demonstratives (that problem)...\n",
    "\n",
    "本课程只关注pronouns..\n",
    "\n",
    "\n",
    "### Motivation\n",
    "essential for deep semantic analysis; useful for QA (reading comprehension)\n",
    "\n",
    "#### Antecedent Restribtions\n",
    "1. Pronouns must agree in number with their antecedents\n",
    "2. must agree in gender (???)\n",
    "3. the same syntactic clause must be reflexive (...self)\n",
    "\n",
    "#### Preferences (softer restrictions)\n",
    "1. should be recent\n",
    "2. should be salient, as determined by grammatical position\n",
    "> Subject > object > argument of preposition\n",
    "\n",
    "### Supervised Anaphor Resotluion\n",
    "binary classifire for (pronoun, entity) pairs\n",
    "\n",
    "convert restrictions and preferences into features\n",
    "- Binary features for number/gender compatibility \n",
    "- Position of entity in text \n",
    "- Include features about type of entity \n",
    "- words around pronoun and entity\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Anaphora Resolution Tools\n",
    "Stanford CoreNLP (includes pronoun resolution models)\n",
    "\n",
    "rule-based system isn't too bad, and considerably faster than neural models.\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c8f9177a0acce8018d23d7772672ff7f2c1807cf103258a4b51e26a443b2e37b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
