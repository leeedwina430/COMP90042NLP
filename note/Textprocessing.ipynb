{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text processing\n",
    "\n",
    "## Definitions\n",
    "$\\subset$\n",
    "\n",
    "words, Sentence, Document, corus\n",
    "\n",
    "Word Token: each instance of a word\n",
    "Word Type: distinct words ($<<$ Word Token) (???)\n",
    "\n",
    "## preprocessing\n",
    "\n",
    "Break documents (corpus) into individual components (words).\n",
    "\n",
    "### Steps\n",
    "\n",
    "1. remove formatting (\\<p> Hi .. \\<p>)\n",
    "2. sentence segmentation  break documents into sentences\n",
    "3. word tokenisation  break sentences into words\n",
    "4. word normalisation  transform into canonical forms (lower cases)\n",
    "5. stopword removal (function words, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Segmentation\n",
    "\n",
    "### punctuations\n",
    "\n",
    "problems   abbreviations\n",
    "\n",
    "### regex to require capital\n",
    "\n",
    "problems   Mr. Brown....\n",
    "\n",
    "### Have lexicons\n",
    "\n",
    "problems   difficult to enumerate all names and abbreviations\n",
    "\n",
    "### state of the art\n",
    "\n",
    "machine learning, not rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning way   Binary Classifier\n",
    "\n",
    "### .  +  Decision trees, logistic regression\n",
    "\n",
    "### Features   \n",
    "\n",
    "### word shapes\n",
    "\n",
    "e.g. uppercase, lowercase, ALLCAps, number\n",
    "Character length\n",
    "\n",
    "### part of speech tag (词性标注)\n",
    "\n",
    "determiners tend too start a sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Tokenization\n",
    "\n",
    "### English\n",
    "separate out aphabetic strings(\\w+), Abbreviations, Hyphens, Numbers, Dates, Clitics (n t, can t), internet language, Multiword units (New Zealand) \n",
    "\n",
    "### Chinaese\n",
    "\n",
    "#### Existing vocabulary   MaxMatch algorithm\n",
    "\n",
    "Greedily match longest word in the vocabulary\n",
    "\n",
    "$$V= \\{墨,大,的,学,生,与,众,不,同, 墨大,学生,不同,与众不同\\} $$\n",
    "\n",
    "termination at one character, or 2~4 character....\n",
    "\n",
    "doesn t always work (去 买 新西兰 花)\n",
    "\n",
    "Including forward and reverse versions, or machine learning tricks...\n",
    "\n",
    "### German\n",
    "\n",
    "Requires Comound splitter\n",
    "\n",
    "\n",
    "## Subword Tokenisation\n",
    "chatGpT uses...\n",
    "\n",
    "### BpE (byte pair encoding)\n",
    "iteratively merge frequent pairs of characters\n",
    "\n",
    "Colourless green ideas sleep furiously → \n",
    "[colour] [less] [green] [idea] [s] [sleep] [furious] [ly]\n",
    "\n",
    "#### pro\n",
    "\n",
    "1. data informed tokenisation\n",
    "2. works for different languages\n",
    "3. deals better with unkown words\n",
    "\n",
    "#### Examples\n",
    "\n",
    "需要提前已经存储了vocabulary\n",
    "\n",
    "#### cons\n",
    "1. thousands of merges will be run\n",
    "\n",
    "2. most frequent words will be reresented as full words, rarer words will be broken into subwords, even individual letter as to unknown words....\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Normalisation\n",
    "lower casing, removing morphology (cooking cook), spelling correction, expand avvreviations (!!!!)\n",
    "\n",
    "### goal \n",
    "1. reduce vocabulary\n",
    "2. maps words into the same type (capture the meanings)\n",
    "\n",
    "### Inflectional Morphology\n",
    "Nouns, Verbs, Adjectives\n",
    "\n",
    "richer inflectional morphology in other languages\n",
    "\n",
    "#### Solution   Lemmatisation\n",
    "remove inflection to lemma\n",
    "\n",
    "irregularities in English\n",
    "\n",
    "A lexicon of lemmas needed for accurate lemmatisation\n",
    "\n",
    "可以直接掉包...\n",
    "\n",
    "### Derivational Morphology\n",
    "create distinct words, suffixes or prefixes change the lexical\n",
    "\n",
    "#### solution   Stemming\n",
    "strips off all suffixes, leaving a stem. Often not an actual lexical item.\n",
    "\n",
    "1. even less lexical sparsity than lemmatisation\n",
    "2. popular in information retrieval\n",
    "3. not always interpretable\n",
    "\n",
    "##### The porter Stemmer\n",
    "\n",
    "1. first stri inflectional suffixes\n",
    "2. then derivational suffixes \n",
    "\n",
    "c (lowercase) = consonant; e.g. ‘b’, ‘c’, ‘d’ \n",
    "\n",
    "v (lowercase) = vowel; e.g. ‘a’, ‘e’, ‘i’, ‘o’, ‘u’\n",
    "\n",
    "C = a sequence of consonants\n",
    "\n",
    "V = a sequence of vowels\n",
    "\n",
    "A word has one of the four forms:\n",
    "CVCV...C, CVCV...V, VCVC...C, VCVC...V => [C]VCVC … [V] or [C] (VC)m [V]\n",
    "\n",
    "m = measure\n",
    "\n",
    "Rules format  S1 => S2. always using the longest matching\n",
    "\n",
    "###### processing\n",
    "1. plurals and inflectional morphology\n",
    "2~4. derivational inflections\n",
    "5. tidying up\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopword Removal\n",
    "\n",
    "### Definition of Stopwords\n",
    "not appropriate when sequence is important\n",
    "\n",
    "high frequency words\n",
    "\n",
    "NLTK, spaCy NLp toolkits\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c8f9177a0acce8018d23d7772672ff7f2c1807cf103258a4b51e26a443b2e37b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
