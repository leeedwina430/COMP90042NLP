{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hidden Markov Models\n",
    "a better approach then POS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Tagging Recap\n",
    "Pann Treebank...\n",
    "\n",
    "cons: prone to error propagation (local classifier)\n",
    "\n",
    "solution: treat the full sequence as a \"class\"? \n",
    "\n",
    "cons(again): exponentially many combinations $|Tags|^M$ for length $M$; different lengths sequences? tagging \n",
    "\n",
    "### A beeter approach\n",
    "decompose it into small word-level tasks\n",
    "\n",
    "1. decomposes process into individual word level steps\n",
    "2. take into accout the whole sequence when learning ??\n",
    "\n",
    "**Sequence labelling, or structured prediction**: also exsists in protein fields (out of nlp)...\n",
    "\n",
    "\n",
    "### HMM: A probabilistic model\n",
    "Goal: obtain best tag sequence $t$ from sentence $w$\n",
    "$$\\hat{t}=\\argmax_t P(t|w)$$\n",
    "with bayes rule (with argmax)\n",
    "$$\\hat{t}=\\argmax_t \\frac{P(w|t)P(t)}{P(w)}=argmax_t P(w|t)P(t)$$\n",
    "\n",
    "#### Assumptions\n",
    "1. **Output independence**: prob of a word (**observed event**) depends only on the tag (**hidden state**) (have nothing with previous words)\n",
    "\n",
    "$$P(w|t)=\\prod_{i=1}^nP(w_i|t_i)$$\n",
    "\n",
    "really strong assumption; simple form\n",
    "\n",
    "2. **Markov assumption**: prob of a tag (**current hidden state**) depends only on the previous tag (**previous hidden state**)\n",
    "\n",
    "$$P(t)=\\prod_{i=1}^nP(t_i|t_{i-1})$$\n",
    "\n",
    "(silimar to bigram models; called bigram tag)\n",
    "\n",
    "### HMMs-Training\n",
    "Emission (o) probabilities\n",
    "$$P(w_i|t_i)=Emission (o)=\\frac{count(t_i,w_i)}{count(t_i)}$$\n",
    "\n",
    "transition (A) probabilities\n",
    "$$P(t_i|t_i)=transition (A)=\\frac{count(t_i,t_{i-1})}{count(t_{i-1})}$$\n",
    "\n",
    "construct two matrix (co-occurence of tag & tag or tag & word); Dimension can be very large; for emission matrix, can ba <font color=red>sparse</font>; row condition, i.e., sum of rows are 1s\n",
    "\n",
    "\n",
    "training with MLE (just as g-gram LMs): counting frequencies\n",
    "\n",
    "#### Start of a sentence\n",
    "Add $<s>$ to represent the start of a sentence\n",
    "\n",
    "$$P(w|<s>)=\\frac{count(<s>,w)}{count(<s>)}$$\n",
    "\n",
    "#### Smoothing with unseen cases\n",
    "same as n-gram LMs\n",
    "\n",
    "\n",
    "### HMMs-Prediction (Decoding)\n",
    "$$\\hat{t}=\\argmax_t P(w|t)P(t)$$\n",
    "\n",
    "**Greedy**: for each word, take the tag $t_i$ that maximises $$, left-to-right; <font color=red>wrong</font>\n",
    "\n",
    "**should consider all possible tag conbinations**\n",
    "\n",
    "\n",
    "### the Viterbi Algorithm\n",
    "**DP** algorithm; \n",
    "\n",
    "<img src=\"\">\n",
    "\n",
    "keep track of scores for each tag for “can” and check them with the different tags of “play\"\n",
    "\n",
    "Process:\n",
    "1. start from left, comput $P(word|tag_i)P(tag_i|tag_{i-1})$, record all\n",
    "2. \n",
    "\n",
    "**Complexity**: $O(T^2N)$, $T$ the size of tagset, $N$ the length of the sequence.\n",
    "\n",
    "\n",
    "1. works cause the <font color=red>independence assumptions</font> made by HMM\n",
    "2. work with <font color=red>log prob</font>\n",
    "3. <font color=red>matrix, or vectorisation</font>\n",
    "\n",
    "e.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10079999999999999 0.05039999999999999 0.0546 0.0468\n"
     ]
    }
   ],
   "source": [
    "a =0.8*0.3*0.42\n",
    "b=0.2*0.6*0.42\n",
    "\n",
    "c=0.7*0.3*0.26\n",
    "d=0.3*0.6*0.26\n",
    "\n",
    "print(a,b,c,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = np.zers(M,T)\n",
    "for t in range(T):\n",
    "    alpha[1,t] = pi[t] * o[w[1],t]\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "88279d2366fe020547cde40dd65aa0e3aa662a6ec1f3ca12d88834876c85e1a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
